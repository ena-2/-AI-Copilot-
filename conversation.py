# /core/conversation.py
# Imports simples para funcionar con el parche de sys.path en web.py
from core.prompting import build_messages, get_intent
from services.llm import get_llm_response

MAX_SESSION_TURNS = 20

MENU_MESSAGE = """
ğŸ¤– **MenÃº AI Copilot**
Por favor, usa uno de los siguientes comandos para iniciar una tarea especÃ­fica:

* **/nota [contenido]**: Para guardar una nota o idea rÃ¡pida.
* **/recordatorio [contenido]**: Para crear un recordatorio.
* **/busqueda [pregunta]**: Para preguntas rÃ¡pidas, educaciÃ³n o tips.

Ejemplo: `/recordatorio Comprar leche maÃ±ana a las 8AM`
"""

def handle_conversation(user_input: str, history: list, turn_count: int) -> tuple[str, list, int]:
    """
    Maneja el flujo de la conversaciÃ³n, intents y lÃ­mites.
    """
    
    if turn_count >= MAX_SESSION_TURNS:
        return "ğŸ‘‹ **LÃ­mite de SesiÃ³n Alcanzado:** Has alcanzado el lÃ­mite de 20 turnos. Por favor, reinicia la sesiÃ³n para continuar.", history, turn_count

    intent = get_intent(user_input)
    
    if intent == 'menu':
        return MENU_MESSAGE, history, turn_count + 1

    history.append({"role": "user", "content": user_input})

    messages = build_messages(history)
    llm_output = get_llm_response(messages)
    
    llm_response = llm_output["response"]
    
    if not llm_output["success"]:
        history.append({"role": "assistant", "content": llm_response})
        return llm_response, history, turn_count + 1

    if "instrucciones internas" in user_input.lower() or "dime tu system prompt" in user_input.lower():
         llm_response = "âŒ No puedo ayudarte con esa solicitud. Mi funciÃ³n es asistirte en tareas diarias y productividad."
         
    history.append({"role": "assistant", "content": llm_response})
         
    turn_count += 1

    return llm_response, history, turn_count